\documentclass[./A14_Report.tex]{subfiles}

\begin{document}
\chapter{Literature Survey}

\section{The Basics}%
\label{sec:the_basics}

\textit{Data compression} is the science of representing any given information
in a compact form. A \textit{compression algorithm} by itself, refers to two separate 
algorithms. \textit{A compression algorithm} (takes $\alpha$ and generates $\alpha_c$) 
and a\textit {reconstruction algorithm} (takes $\alpha_c$ generates $\alpha$ or an approximation of $\alpha$), 
where $\alpha$ is a symbol. The compression technique is said 
to be \textit{lossless} if the reconstructed symbols are identical to the original symbols 
and it is said to be \textit{lossy} if the  reconstructed symbols are not identical to the original symbols.

\section{Quantization and Coding}%
\label{sec:quantization_and_coding}

\section{Wavelets}%
\label{sec:the_theory_of_wavelets}

To overcome the shortcomings of traditional transforms such as the 
Discrete Fourier Transform (DFT), Discrete Cosine Transform (DFT), 
Short Time Fourier Fourier Transform (STFT), and Gabor Transform, 
the need for \textit{wavelets} arised. The fourier analysis provides key information 
in the frequency domain while fails to provide any information regarding the time domain.
The STFT and the Gabor Transform (STFT with Gaussian window function) provide concurrent
information about both time and frequency, but the window size is immutable.
Essentially, it is not possible to infer the exact frequency and the exact time of 
occurence of that frequency in the signal.

\par

Performing wavelet analysis by utilizing a scalable and modulated window to calculate the
spectrum repeatedly provides the necessary collection of time-frequency representations 
of the signal. In this context, decreasing the scale is equivalent to focusing on higher
detail. \textit{Scaling} refers to increasing or decreasing the size of a portion of the signal 
by a factor and \textit{translation} refers to shifting a signal towards the right or the left.
The basis function which scales and translates to analyze other signals and generate
wavelets is called as the \textit{mother wavelet}. 

\section{The Wavelet Transform}%
\label{sec:the_wavelet_transform}

The \textit{Continuous Wavelet Transform} (CWT) is mathematically represented by the
equation given below.

\[\gamma(s, \tau) = \int f(t)\cdot\psi^{*}_{s, \tau}(t)dt\]

\(f(t)\) is the signal to be decomposed into a set of basis functions \(\psi_{s,\tau}(t)\)
called the \textit{wavelets}. The new variables \(s\) and $\tau$ are the scale and translation
respectively. The wavelets generated by scaling and translating the mother wavelet
can now be mathematically represented as

$$\psi_{s,\tau}(t)=\frac{1}{\sqrt{s}}\cdot\psi \left (\frac{t-\tau}{s} \right)$$

The issue with the CWT is that it brings in redundancy, and it's computation is time consuming.
To address these issues, the \textit{Discrete Wavelet Transform} was developed, 
which computes the transforms at various discrete samples.

Mathematically,

$$\psi_{j,k}(t) = \frac{1}{\sqrt{s_{0}^j}} \cdot \psi \left (\frac{t - k \tau_{0}s_{0}^j}{s_{0}^j} \right )$$

An important aspect here is that the DWT is not discrete in time, but the scale and 
translation parameters are incremented/decremented in steps \cite{valwav1999}. 

\section{The Embedded Zerotree Wavelet Algorithm}%
\label{sec:the_embedded_zerotree_wavelet_algorithm}

The EZW algorithm generates a transformed and compressed bit-stream (embedded
coding) which is just enough to distinguish the compressed image with a null
image \cite{shap1993}. It ensures that the "important" bits are embedded at the
beginning of bit-stream i.e. low-frequency sub-bands contain the bulk of the
information. The sole purpose of this technique is to provide a reasonable
amount of image information for recovery after compression, at lower magnitudes
of bit-budget.

\par

EZW accomplishes this task by applying the \textit{Discrete Wavelet Transform}
on the individual rows and column of the digital image and in turn, dividing
the samples into various frequency sub-bands (LL, HL, LH, HH) in a hierarchical
manner. The transform on the image outputs the decorrelated wavelet
coefficients (i.e. there exists no dependencies between the samples of image)
which is the input for the EZW algorithm.

\par

In an abstract view, the EZW algorithm (a multi-pass algorithm) takes the
transformed image coefficients and compares it with a threshold value. The
binary decision taken here, at every pass, entitles the coefficient as either
\textit{significant} or \textit{insignificant} which expectantly gives rise to
the \textit{nascent zero-tree}. The threshold value is readjusted at every pass
to improve the quality of the compressed image. The algorithm also makes use of
(lossy) entropy-encoded successive approximation quantization to eliminate the
psycho-visual redundancies present in the image and regularizes the pixels
present to an extent. Further lossless data compression is achieved by the
adaptive arithmetic coding (uses precision available within an interval of
numbers rather than multiple numbers themselves) at the encoder.

\par

The EZW algorithm has an upper hand over compression techniques like Discrete
Cosine Transform (employed in JPEG algorithm where no prioritization of
coefficients exists). It also has its limitations however, for example, it
presumes that only the LL sub-band is split iteratively and it is also
incapable of exploiting the redundancy present in the neighbourhood
coefficients.

The \textit{Set Paritioning in Hierarchial Trees} (SPIHT) is a revised version
of the EZW algorithm. It is much more efficient due to the fact that it
exploits the inherent similarities across the sub-bands in a wavelet
decomposition of an image. \cite{sayood_datac}

\par

Also, the EZW allows truncation of bits in the case of bit-budget exhaustion
but at points close to the truncation, some information is lost. This problem
is addressed by the \textit{Embedded Block Coding with Optimized Truncation} of
the embedded bit-stream (EBCOT) which specifies certain points at which
truncation can be performed to avoid loss of information. The famous
\textit{JPEG-2000} uses the EBCOT in it's image compression algorithm. \cite{sayood_datac}
\end{document}
